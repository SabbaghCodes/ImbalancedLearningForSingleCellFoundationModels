{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92ba312",
   "metadata": {},
   "source": [
    "# scBERT\n",
    "This is a modified version of the code provided by the scBERT authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2055d07c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7d191f-2be5-4a8f-9905-836eca72a54c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import argparse\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import random\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, classification_report\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingWarmRestarts, CyclicLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from performer_pytorch import PerformerLM\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "from utils import *\n",
    "import pickle as pkl\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c161b5a4",
   "metadata": {},
   "source": [
    "## Hyperparameter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89313004-ba39-4480-82e8-1270ea731182",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION = 60\n",
    "LEARNING_RATE = 1e-4\n",
    "SEQ_LEN = 3001\n",
    "VALIDATE_EVERY = 1\n",
    "TEST_EVERY = 1\n",
    "LOSS = 'ce'\n",
    "OPTIMIZER = 'adam' \n",
    "\n",
    "PATIENCE = 10\n",
    "UNASSIGN_THRES = 0.0\n",
    "\n",
    "CLASS = 7\n",
    "POS_EMBED_USING = True\n",
    "\n",
    "model_name = 'ms_default'\n",
    "modelraw = model_name\n",
    "ckpt_dir = './results/'\n",
    "data_path = 'ms_default.h5ad'\n",
    "test_path = 'ms_test.h5ad'\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcabbaf5",
   "metadata": {},
   "source": [
    "## Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dfffe7-cd47-49ed-a379-6f15f12177d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = random.randint(0, self.data.shape[0]-1)\n",
    "        full_seq = self.data[rand_start].toarray()[0]\n",
    "        full_seq[full_seq > (CLASS - 2)] = CLASS - 2\n",
    "        full_seq = torch.from_numpy(full_seq).long()\n",
    "        full_seq = torch.cat((full_seq, torch.tensor([0]))).to(device)\n",
    "        seq_label = self.label[rand_start]\n",
    "        return full_seq, seq_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self, dropout = 0., h_dim = 100, out_dim = 10):\n",
    "        super(Identity, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, (1, 200))\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(in_features=SEQ_LEN, out_features=512, bias=True)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=h_dim, bias=True)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc3 = nn.Linear(in_features=h_dim, out_features=out_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:,None,:,:]\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, num_classes = 7, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.num_classes = num_classes\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        effective_target = torch.eye(self.num_classes)[targets.to('cpu')]\n",
    "\n",
    "        # Calculate Cross entropy\n",
    "        logit = F.softmax(inputs, dim=1)\n",
    "        logit = logit.clamp(1e-7, 1.0) \n",
    "        ce = -(effective_target * torch.log(logit.to('cpu')))\n",
    "\n",
    "        # Calculate Focal Loss\n",
    "        weight = torch.pow(-logit + 1., self.gamma)\n",
    "        fl = ce * weight.to('cpu') * self.alpha\n",
    "\n",
    "        if self.reduction == 'sum':\n",
    "            return fl.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            return fl.mean()\n",
    "\n",
    "class LDAMLoss(nn.Module):\n",
    "    def __init__(self, cls_num_list, max_m=0.5, weight=None, s=30):\n",
    "        super(LDAMLoss, self).__init__()\n",
    "        m_list = 1.0 / np.sqrt(np.sqrt(cls_num_list))\n",
    "        m_list = m_list * (max_m / np.max(m_list))\n",
    "        m_list = torch.cuda.FloatTensor(m_list)\n",
    "        self.m_list = m_list\n",
    "        assert s > 0\n",
    "        self.s = s\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        index = torch.zeros_like(x, dtype=torch.uint8)\n",
    "        index.scatter_(1, target.data.view(-1, 1), 1)\n",
    "        \n",
    "        index_float = index.type(torch.cuda.FloatTensor)\n",
    "        batch_m = torch.matmul(self.m_list[None, :], index_float.transpose(0,1))\n",
    "        batch_m = batch_m.view((-1, 1))\n",
    "        x_m = x - batch_m\n",
    "    \n",
    "        output = torch.where(index, x_m, x)\n",
    "        return F.cross_entropy(self.s*output, target, weight=self.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e22251",
   "metadata": {},
   "source": [
    "## Data Setup, Training, and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b23fad-7461-4780-8c72-3203ad044bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataraw = sc.read_h5ad(data_path)\n",
    "test_data = sc.read_h5ad(test_path)\n",
    "label_dict, label = np.unique(np.array(dataraw.obs['celltype']), return_inverse=True)\n",
    "with open('label_dict', 'wb') as fp:\n",
    "    pkl.dump(label_dict, fp)\n",
    "with open('label', 'wb') as fp:\n",
    "    pkl.dump(label, fp)\n",
    "class_num = np.unique(label, return_counts=True)[1].tolist()\n",
    "class_weight = torch.tensor([(1 - (x / sum(class_num))) ** 2 for x in class_num])\n",
    "label = torch.from_numpy(label)\n",
    "dataraw = dataraw.X\n",
    "\n",
    "acc = []\n",
    "f1 = []\n",
    "f1w = []\n",
    "pred_list = pd.Series(['un'] * dataraw.shape[0])\n",
    "test_dataset = SCDataset(test_data.X, test_data.obs['celltype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80cbbd0-e33f-4395-861a-df4b267cb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_split = 0\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=SEED)\n",
    "for index_train, index_val in sss.split(dataraw, label):\n",
    "    number_split = number_split + 1\n",
    "    print(f'Start of split: {number_split}')\n",
    "    \n",
    "    model_name = modelraw + str(number_split)\n",
    "    data_train, label_train = dataraw[index_train], label[index_train]\n",
    "    data_val, label_val = dataraw[index_val], label[index_val]\n",
    "    train_dataset = SCDataset(data_train, label_train)\n",
    "    val_dataset = SCDataset(data_val, label_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "    model = PerformerLM(\n",
    "        num_tokens = CLASS,\n",
    "        dim = 200,\n",
    "        depth = 6,\n",
    "        max_seq_len = SEQ_LEN,\n",
    "        heads = 10,\n",
    "        local_attn_heads = 0,\n",
    "        g2v_position_emb = POS_EMBED_USING\n",
    "    )\n",
    "\n",
    "    path = args.model_path\n",
    "    ckpt = torch.load(path)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.norm.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.performer.net.layers[-2].parameters():\n",
    "        param.requires_grad = True\n",
    "    model.to_out = Identity(dropout=0., h_dim=128, out_dim=label_dict.shape[0])\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    if OPTIMIZER == 'adam':\n",
    "        optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    elif OPTIMIZER == 'adamw':\n",
    "        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    elif OPTIMIZER == 'sgd':\n",
    "        optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    else:\n",
    "        raise ValueError(f'Optimizer \\'{OPTIMIZER}\\' is not a valid option.')\n",
    "        \n",
    "    scheduler = CosineAnnealingWarmupRestarts(\n",
    "        optimizer,\n",
    "        first_cycle_steps=15,\n",
    "        cycle_mult=2,\n",
    "        max_lr=LEARNING_RATE,\n",
    "        min_lr=1e-6,\n",
    "        warmup_steps=5,\n",
    "        gamma=0.9\n",
    "    )\n",
    "    \n",
    "    if LOSS == 'ce':\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=None).to(device)\n",
    "    elif LOSS == 'focal':\n",
    "        loss_fn = FocalLoss(alpha=1, gamma=2, num_classes=len(label_dict)).to(device)\n",
    "    elif LOSS == 'ldam':\n",
    "        loss_fn = LDAMLoss(label, max_m=0.5, s=30).to(device)\n",
    "    else:\n",
    "        raise ValueError(f'Loss function \\'{OPTIMIZER}\\' is not a valid option.')\n",
    "                     \n",
    "    trigger_times = 0\n",
    "    max_acc = 0.0\n",
    "    for i in range(1, EPOCHS+1):\n",
    "        \n",
    "        tik = time.time()\n",
    "\n",
    "        train_loader.sampler.set_epoch(i)\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        cum_acc = 0.0\n",
    "        for index, (data, labels) in enumerate(train_loader):\n",
    "            index += 1\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            if index % GRADIENT_ACCUMULATION != 0:\n",
    "                with model.no_sync():\n",
    "                    logits = model(data)\n",
    "                    loss = loss_fn(logits, labels)\n",
    "                    loss.backward()\n",
    "            if index % GRADIENT_ACCUMULATION == 0:\n",
    "                logits = model(data)\n",
    "                loss = loss_fn(logits, labels)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), int(1e6))\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            softmax = nn.Softmax(dim=-1)\n",
    "            final = softmax(logits)\n",
    "            final = final.argmax(dim=-1)\n",
    "            pred_num = labels.size(0)\n",
    "            correct_num = torch.eq(final, labels).sum(dim=-1)\n",
    "            cum_acc += torch.true_divide(correct_num, pred_num).mean().item()\n",
    "        epoch_loss = running_loss / index\n",
    "        epoch_acc = 100 * cum_acc / index\n",
    "        tok = time.time()\n",
    "        train_time = tok - tik \n",
    "        max_memory_reserved = torch.cuda.max_memory_reserved() / 1e9\n",
    "\n",
    "        saving_dir = os.path.join(args.res_dir, model_name, 'train', f'split_{number_split}')\n",
    "        if not os.path.exists(saving_dir):\n",
    "            os.makedirs(saving_dir)\n",
    "\n",
    "        resources_usage_df_path = os.path.join(saving_dir, f'resources_usage.csv')\n",
    "        if os.path.exists(resources_usage_df_path):\n",
    "            resources_usage_df = pd.read_csv(resources_usage_df_path)\n",
    "        else:\n",
    "            resources_usage_df = pd.DataFrame(columns=['time_per_epoch', 'memory_reserved', 'train_loss'])\n",
    "        \n",
    "        \n",
    "        resources_usage_df = pd.concat([resources_usage_df, pd.DataFrame({'time_per_epoch':[train_time], 'memory_reserved':[max_memory_reserved], 'train_loss':[epoch_loss]})])\n",
    "        resources_usage_df.to_csv(resources_usage_df_path, index=False)\n",
    "\n",
    "\n",
    "        if is_master:\n",
    "            print(f'    ==  Epoch: {i} | Training Loss: {epoch_loss:.6f} | Accuracy: {epoch_acc:6.4f}%  ==')\n",
    "            print(f'Time: {train_time}')\n",
    "        scheduler.step()\n",
    "\n",
    "        if i % VALIDATE_EVERY == 0:\n",
    "            model.eval()\n",
    "            running_loss = 0.0\n",
    "            predictions = []\n",
    "            truths = []\n",
    "            with torch.no_grad():\n",
    "                for index, (data_v, labels_v) in enumerate(val_loader):\n",
    "                    index += 1\n",
    "                    data_v, labels_v = data_v.to(device), labels_v.to(device)\n",
    "                    logits = model(data_v)\n",
    "                    loss = loss_fn(logits, labels_v)\n",
    "                    running_loss += loss.item()\n",
    "                    softmax = nn.Softmax(dim=-1)\n",
    "                    final_prob = softmax(logits)\n",
    "                    final = final_prob.argmax(dim=-1)\n",
    "                    final[np.amax(np.array(final_prob.cpu()), axis=-1) < UNASSIGN_THRES] = -1\n",
    "                    predictions.append(final)\n",
    "                    truths.append(labels_v)\n",
    "                del data_v, labels_v, logits, final_prob, final\n",
    "                # gather\n",
    "                predictions = torch.cat(predictions, dim=0)\n",
    "                truths = torch.cat(truths, dim=0)\n",
    "                no_drop = predictions != -1\n",
    "                predictions = np.array((predictions[no_drop]).cpu())\n",
    "                truths = np.array((truths[no_drop]).cpu())\n",
    "                cur_acc = accuracy_score(truths, predictions)\n",
    "                f1 = f1_score(truths, predictions, average='macro')\n",
    "                val_loss = running_loss / index\n",
    "                if is_master:\n",
    "                    print(f'    ==  Epoch: {i} | Validation Loss: {val_loss:.6f} | F1 Score: {f1:.6f}  ==')\n",
    "                    conf_mat = confusion_matrix(truths, predictions)\n",
    "                    report = classification_report(truths, predictions, digits=4) #, target_names=label_dict.tolist()\n",
    "                    report_dict = classification_report(truths, predictions, digits=4, output_dict=True) #, target_names=label_dict.tolist()\n",
    "                    saving_dir = os.path.join(args.res_dir, model_name, 'val', f'split_{number_split}')\n",
    "\n",
    "                    if not os.path.exists(saving_dir):\n",
    "                        os.makedirs(saving_dir)\n",
    "                        \n",
    "                    resources_usage_df_path = os.path.join(saving_dir, f'val_loss.csv')\n",
    "                    if os.path.exists(resources_usage_df_path):\n",
    "                        resources_usage_df = pd.read_csv(resources_usage_df_path)\n",
    "                    else:\n",
    "                        resources_usage_df = pd.DataFrame(columns=['val_loss'])\n",
    "\n",
    "                    resources_usage_df = pd.concat([resources_usage_df, pd.DataFrame({'val_loss':[val_loss]})])\n",
    "                    resources_usage_df.to_csv(resources_usage_df_path, index=False)\n",
    "                    pd.DataFrame(conf_mat).to_csv(os.path.join(saving_dir, f'conf_mat_E{i}.csv'), index=False)\n",
    "                    pd.DataFrame(report_dict).transpose().to_csv(os.path.join(saving_dir, f'report_E{i}.csv'), index=False)\n",
    "\n",
    "                    print(conf_mat)\n",
    "                    print(report)\n",
    "                    print(label_to_index)\n",
    "                if cur_acc > max_acc:\n",
    "                    max_acc = cur_acc\n",
    "                    trigger_times = 0\n",
    "                    save_best_ckpt(i, number_split, model, optimizer, scheduler, val_loss, model_name, os.path.join(ckpt_dir, 'val'), predictions)\n",
    "                else:\n",
    "                    trigger_times += 1\n",
    "                    if trigger_times > PATIENCE:\n",
    "                        break\n",
    "\n",
    "        if i % TEST_EVERY == 0:\n",
    "            model.eval()\n",
    "            running_loss = 0.0\n",
    "            predictions = []\n",
    "            truths = []\n",
    "            with torch.no_grad():\n",
    "                for index, (data_t, labels_t) in enumerate(test_loader):\n",
    "                    index += 1\n",
    "                    print(labels_t)\n",
    "                    print(data_t)\n",
    "                    if torch.is_tensor(labels_t):\n",
    "                        data_t, labels_t = data_t.to(device), labels_t.to(device)\n",
    "                    else:\n",
    "                        data_t, labels_t = data_t.to(device), torch.tensor(labels_t).to(device)\n",
    "                    logits = model(data_t)\n",
    "                    loss = loss_fn(logits, labels_t)\n",
    "                    running_loss += loss.item()\n",
    "                    softmax = nn.Softmax(dim=-1)\n",
    "                    final_prob = softmax(logits)\n",
    "                    final = final_prob.argmax(dim=-1)\n",
    "                    final[np.amax(np.array(final_prob.cpu()), axis=-1) < UNASSIGN_THRES] = -1\n",
    "                    predictions.append(final)\n",
    "                    truths.append(labels_t)\n",
    "                del data_t, labels_t, logits, final_prob, final\n",
    "                # gather\n",
    "                predictions = torch.cat(predictions, dim=0)\n",
    "                truths = torch.cat(truths, dim=0)\n",
    "                no_drop = predictions != -1\n",
    "                predictions = np.array((predictions[no_drop]).cpu())\n",
    "                truths = np.array((truths[no_drop]).cpu())\n",
    "                cur_acc = accuracy_score(truths, predictions)\n",
    "                f1 = f1_score(truths, predictions, average='macro')\n",
    "                test_loss = running_loss / index\n",
    "                if is_master:\n",
    "                    print(f'    ==  Epoch: {i} | Test Loss: {test_loss:.6f} | F1 Score: {f1:.6f}  ==')\n",
    "                    conf_mat = confusion_matrix(truths, predictions)\n",
    "                    report = classification_report(truths, predictions, digits=4) #, target_names=label_dict.tolist()\n",
    "                    report_dict = classification_report(truths, predictions, digits=4, output_dict=True) #, target_names=label_dict.tolist()\n",
    "                    saving_dir = os.path.join(args.res_dir, model_name, 'test', f'split_{number_split}')\n",
    "\n",
    "                    if not os.path.exists(saving_dir):\n",
    "                        os.makedirs(saving_dir)\n",
    "                        \n",
    "                    resources_usage_df_path = os.path.join(saving_dir, f'test_loss.csv')\n",
    "                    if os.path.exists(resources_usage_df_path):\n",
    "                        resources_usage_df = pd.read_csv(resources_usage_df_path)\n",
    "                    else:\n",
    "                        resources_usage_df = pd.DataFrame(columns=['test_loss'])\n",
    "\n",
    "                    resources_usage_df = pd.concat([resources_usage_df, pd.DataFrame({'test_loss':[test_loss]})])\n",
    "                    resources_usage_df.to_csv(resources_usage_df_path, index=False)\n",
    "                    \n",
    "                    pd.DataFrame(conf_mat).to_csv(os.path.join(saving_dir, f'conf_mat_E{i}.csv'), index=False)\n",
    "                    pd.DataFrame(report_dict).transpose().to_csv(os.path.join(saving_dir, f'report_E{i}.csv'), index=False)\n",
    "\n",
    "                    print(conf_mat)\n",
    "                    print(report)\n",
    "                if cur_acc > max_acc:\n",
    "                    max_acc = cur_acc\n",
    "                    trigger_times = 0\n",
    "                    save_best_ckpt(i, number_split, model, optimizer, scheduler, test_loss, model_name, os.path.join(ckpt_dir, 'test'), predictions)\n",
    "                else:\n",
    "                    trigger_times += 1\n",
    "                    if trigger_times > PATIENCE:\n",
    "                        break\n",
    "\n",
    "        del predictions, truths\n",
    "        torch.cuda.empty_cache()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
